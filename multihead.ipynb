{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "import transformers\n",
    "from torch import nn\n",
    "import torch\n",
    "from be_great.multihead_models import *\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distil GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hdd2/sonia/miniconda3/envs/great/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgpt2 = transformers.AutoModelForCausalLM.from_pretrained('distilgpt2')\n",
    "dgpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MOEModelForCausalLM(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MOEMLP(\n",
       "          (mlps): ModuleList(\n",
       "            (0-2): 3 x GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_experts = 3\n",
    "dgpt2copy = MOEModelForCausalLM(dgpt2, num_experts=num_experts)\n",
    "dgpt2copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "instring = 'hello'\n",
    "\n",
    "toks = tokenizer(instring, return_tensors='pt')\n",
    "outs = dgpt2.forward(**toks)\n",
    "outsmoe = dgpt2copy.forward(**toks)\n",
    "assert outs.logits.equal(outsmoe[0])\n",
    "assert dgpt2copy.generate(**toks).equal(dgpt2.generate(**toks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hdd2/sonia/miniconda3/envs/great/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/hdd2/sonia/miniconda3/envs/great/lib/python3.11/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hello The U.S. Department of Justice has been investigating the death of a man who was shot']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "instring = 'hello'\n",
    "\n",
    "toks = tokenizer(instring, return_tensors='pt')\n",
    "tokenizer.batch_decode(dgpt2.generate(**toks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.57s/it]\n",
      "/hdd2/sonia/miniconda3/envs/great/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/hdd2/sonia/miniconda3/envs/great/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/hdd2/sonia/miniconda3/envs/great/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/hdd2/sonia/miniconda3/envs/great/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama = transformers.AutoModelForCausalLM.from_pretrained('/zoo/llama2/llama2-7b-hf', torch_dtype=torch.bfloat16)\n",
    "llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep copied model\n",
      "added MOE MLPs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MOEModelForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MOEMLP(\n",
       "          (mlps): ModuleList(\n",
       "            (0): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llamacopy = MOEModelForCausalLM.from_other(llama)\n",
    "llamacopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first assert passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hdd2/sonia/miniconda3/envs/great/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/hdd2/sonia/miniconda3/envs/great/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama generation complete\n",
      "llamacopy generation complete\n",
      "second assert passed\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer \n",
    "tokenizerllama = AutoTokenizer.from_pretrained(\"/zoo/llama2/llama2-7b-hf\")\n",
    "llama.cpu(), llamacopy.cpu()\n",
    "\n",
    "instring = 'hello'\n",
    "toks = tokenizerllama(instring, return_tensors='pt')\n",
    "outs = llama.forward(**toks)\n",
    "outsmoe = llamacopy.forward(**toks)\n",
    "assert outs.logits.equal(outsmoe[0])\n",
    "print('first assert passed')\n",
    "\n",
    "outs = llama.cuda().generate(input_ids=toks['input_ids'].cuda(), attention_mask=toks['attention_mask'].cuda())\n",
    "print('llama generation complete')\n",
    "outsmoe = llamacopy.cuda().generate(input_ids=toks['input_ids'].cuda(), attention_mask=toks['attention_mask'].cuda())\n",
    "print('llamacopy generation complete')\n",
    "assert outsmoe.equal(outs)\n",
    "print('second assert passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dgpt2copy # don't forget to change tokenizer name too\n",
    "\n",
    "model.train()\n",
    "# Move the model to the device (GPU if available)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data stuff\n",
    "# Load the dataset\n",
    "file_path = '/hdd3/sonia/data/adult.csv'  # Update this with the correct path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Preprocess the data: Convert each row to a string\n",
    "def row_to_string(row):\n",
    "    return \", \".join([f\"{col} is {val}\" for col, val in row.items()]) + \".\"\n",
    "def row_to_sentences(row):\n",
    "    return '. '.join([str(col).strip() + \" is \" + str(val).strip() for col, val in zip(row.index, row.values)])\n",
    "def row_to_col_sentences(row):\n",
    "    return [str(col).strip() + \" is \" + str(val).strip() + '.' for col, val in zip(row.index, row.values)]\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, num_experts, max_col_length=10):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_col_length = max_col_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx][:num_experts] # ['age is 39', 'workclass is State-gov', ...]\n",
    "        tokenized_text = self.tokenizer(text, truncation=True, max_length=self.max_col_length, padding='max_length', return_tensors=\"pt\")\n",
    "        return tokenized_text.input_ids.squeeze(), tokenized_text.attention_mask.squeeze()\n",
    "\n",
    "\n",
    "text_data = data.apply(row_to_col_sentences, axis=1).tolist()\n",
    "dataset = TextDataset(text_data, tokenizer, num_experts)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "# Set up the optimizer and learning rate scheduler\n",
    "optimizer = AdamW(dgpt2.parameters(), lr=5e-2)\n",
    "num_training_steps = len(dataloader) * 3  # Number of epochs\n",
    "lr_scheduler = LinearLR(optimizer, total_iters=num_training_steps)\n",
    "\n",
    "ins = tokenizer(tokenizer.bos_token, return_tensors='pt')\n",
    "\n",
    "losses = []\n",
    "for epoch in range(3):  # Train for 3 epochs\n",
    "    for batch in tqdm(dataloader):\n",
    "        print(batch)\n",
    "        labels, labels_mask = batch\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(ins['input_ids'].to(device), ins['attention_mask'].to(device), labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd1944c6f50>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA2FElEQVR4nO3deXxU1cH/8e9MVggzbCEkhASRVaBsAdEYRNGgRisiGhfaIqJo6AJaN/wpYgvBthJxA3wUUajUB2vdqnkaoWgRghiqQtgUEYEAgbAlQMhCzu8PyJAxATMx5ID38369zsvkzrl3zswFz5dzzz3XJckIAADAErftBgAAAGcjjAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwKth2A2qrTZs2Kioqst0MAAAQAI/Ho+3bt5+yzlkRRtq0aaO8vDzbzQAAAHUQGxt7ykByVoSRyhGR2NhYRkcAADhLeDwe5eXl/WDffVaEkUpFRUWEEQAAfmKYwAoAAKwijAAAAKvqFEbS0tK0adMmFRcXKycnR0lJSSetO2fOHBljqpXc3Nw6NxoAAPx0BBxGUlNTNX36dE2ZMkV9+vTRkiVLlJmZqbi4uBrrjxs3TtHR0b7Stm1b7dmzR2+88caPbjwAAPhpMIGU5cuXmxkzZvhtW7t2rUlPT6/V/kOHDjVHjx418fHxtX5Pj8djjDHG4/EE1FYKhUKhUCj2Sm3774BGRkJCQpSQkKCsrCy/7VlZWUpMTKzVMUaPHq2FCxdqy5YtJ60TGhoqj8fjVwAAwE9TQGEkMjJSwcHBys/P99uen5+v6OjoH9w/OjpaV111lV566aVT1pswYYIKCwt9hQXPAAD46arTBFZjjN/vLper2raa3Hbbbdq/f7/efvvtU9abOnWqvF6vr8TGxtalmQAA4CwQ0KJnBQUFKi8vrzYKEhUVVW20pCa333675s2bp7KyslPWKy0tVWlpaSBNAwAAZ6mARkbKysq0cuVKJScn+21PTk7WsmXLTrnvoEGD1KlTJ82ePTvwVgIAgJ+0gGbGpqammpKSEjNq1CjTtWtXk5GRYYqKinx3x6Snp5tXX3212n5z58412dnZp3U2LoVCoVAolDOn1Lb/DvjZNAsWLFDLli01ceJExcTEKDc3VykpKb67Y2JiYhQfH++3j9fr1fDhwzVu3LhA3w4AAPzEuXQslZzRPB6PCgsL5fV66/VBeQk/v0ptu3XR6kUfa1PO5/V2XAAAUPv+29HPpjkv6QJd/IubFNulk+2mAADgWI4OI77bkV0uuw0BAMDBCCOSXG7CCAAAthBGJLlEGAEAwBZHhxH5rtIQRgAAsMXRYcSYimM/EEYAALDG4WGEOSMAANjm6DDCZRoAAOxzdBg58aRhwggAALY4O4xUHJszwsgIAAD2ODuMiDkjAADY5ugw4nsqDyMjAABY4+gw4rubhjACAIA1jg4j8q3ACgAAbHF0GOFBeQAA2EcYkeRyO/prAADAKmf3wswZAQDAOkeHkROXaey2AwAAJ3N2GKmonMBKGgEAwBZnhxExZwQAANuc3Qv75oxYbgcAAA7m6DDCrb0AANjn7DDCnBEAAKxzdBgR64wAAGCdo3vhygmsDIwAAGCPo8MIi54BAGCfo8NI5fxVJrACAGCPs8NIRYUkJrACAGCTs8OIb9EzwggAALY4Ooz45q9ymQYAAGscHUZY9AwAAPucHUYq54wQRgAAsMbRYYRbewEAsM/RYaTyzl4u0wAAYI+zwwgjIwAAWOfsMHJ8zgjLjAAAYI+jw0jldRq3y9lfAwAANjm6F+bWXgAA7HN0GKkcGmHOCAAA9jg6jJwYGbHbDgAAnMzZYaSCkREAAGxzdBg5seiZs78GAABsqlMvnJaWpk2bNqm4uFg5OTlKSko6Zf3Q0FBNnjxZmzdv1pEjR7Rx40aNGjWqTg2uT1ymAQDAvuBAd0hNTdX06dM1duxYLV26VHfddZcyMzPVrVs3bd26tcZ9FixYoNatW2v06NHauHGjoqKiFBwc8FvXO8MEVgAAzggmkLJ8+XIzY8YMv21r16416enpNda/4oorzL59+0zz5s0Dep+qxePxGGOM8Xg8dT5GTWXgL24y01ZnmxFPTKrX41IoFAqFQql9/x3QZZqQkBAlJCQoKyvLb3tWVpYSExNr3Ofaa69VTk6OHnjgAW3btk0bNmzQX/7yF4WHh5/0fUJDQ+XxePzKacFy8AAAWBfQtZLIyEgFBwcrPz/fb3t+fr6io6Nr3Ofcc89VUlKSjhw5omHDhikyMlIzZsxQixYtNHr06Br3mTBhgiZNmhRI0+qERc8AALCvThNYfZ34cS6Xq9o23xu43TLGaMSIEfrss8+UmZmpe++9V7fddttJR0emTp0qr9frK7GxsXVpZi0wMgIAgG0BjYwUFBSovLy82ihIVFRUtdGSSjt27FBeXp4KCwt929atWye32622bdtq48aN1fYpLS1VaWlpIE2rk8p1RhgZAQDAnoBGRsrKyrRy5UolJyf7bU9OTtayZctq3Gfp0qVq06aNIiIifNs6d+6so0ePatu2bXVocv0xzBkBAMC6gC/TZGRk6I477tCoUaPUtWtXZWRkKD4+XrNmzZIkpaen69VXX/XVnz9/vvbs2aM5c+bovPPO08CBA/WXv/xFL7/8so4cOVJ/n6QOCCMAANgX8GIfCxYsUMuWLTVx4kTFxMQoNzdXKSkp2rJliyQpJiZG8fHxvvqHDh1ScnKynn32WeXk5GjPnj1asGCBHnnkkfr7FHVVOc2FMAIAgDV1Wnls5syZmjlzZo2v1bSy6oYNGzRkyJC6vNVpZUyFJEZGAACwydEPZfFdpnETRgAAsMXRYcS36BkPpwEAwBpHhxHDnBEAAKxzdBhhOXgAAOxzdBipnMDKVRoAAOxxeBg59l+X29FfAwAAVjm6FzZMYAUAwDpHh5ETc0YstwMAAAdzdBg5MWeENAIAgC0ODyPH/sucEQAA7HF2L8ycEQAArHN0GDG+oRG77QAAwMkII2LRMwAAbCKMiDkjAADY5OxemMs0AABY5+gwwqJnAADY5+gwwoPyAACwz9Fh5MTdNIQRAABsIYxIcrkJIwAA2OLoMMKiZwAA2OfoMFJ5lYbLNAAA2OPsMFJx7EF5TGAFAMAeZ4cRMWcEAADbHB1GxN00AABY5+gwcmIBVsIIAAC2ODuMMGcEAADrHB1GxJwRAACsc3QY4dZeAADsc3QY4dk0AADY5+gw4ns2DQAAsMbZYYQJrAAAWOfsMHL8vy63o78GAACscnYvzJwRAACsc3QYYc4IAAD2OTuMVDAyAgCAbY4OI77LNMwZAQDAGkf3wpVP7eXRNAAA2OPsMMIEVgAArHN2GKnw3dxrtR0AADiZo8PIiTkjhBEAAGxxdBjhMg0AAPY5OoxUrsFKGAEAwJ46hZG0tDRt2rRJxcXFysnJUVJS0knrDho0SMaYaqVLly51bnR98S16RhgBAMCagMNIamqqpk+frilTpqhPnz5asmSJMjMzFRcXd8r9OnfurOjoaF/5+uuv69zo+sKiZwAA2BdwGLn33ns1e/ZszZ49W+vXr9c999yjrVu3Ki0t7ZT77dq1S/n5+b5ScfyJuTYxZwQAAPsCCiMhISFKSEhQVlaW3/asrCwlJiaect/PP/9c27dv18KFC3XJJZecsm5oaKg8Ho9fOT24TAMAgG0BhZHIyEgFBwcrPz/fb3t+fr6io6Nr3GfHjh268847NXz4cF1//fXasGGDFi1apIEDB570fSZMmKDCwkJfycvLC6SZtXZiyghhBAAAW4LrstP3n3brcrlO+gTcr776Sl999ZXv9+XLlysuLk733XeflixZUuM+U6dOVUZGhu93j8dzWgKJOQMuFQEA4HQBjYwUFBSovLy82ihIVFRUtdGSU1m+fLk6dep00tdLS0tVVFTkV06L4wHKzYPyAACwJqBeuKysTCtXrlRycrLf9uTkZC1btqzWx+nTp4927NgRyFufFtzaCwCAfQFfpsnIyNC8efOUk5Oj7OxsjRkzRvHx8Zo1a5YkKT09XbGxsRo5cqQkady4cdq8ebPWrFmj0NBQ/eIXv9ANN9yg66+/vn4/yY/AnBEAAOwJOIwsWLBALVu21MSJExUTE6Pc3FylpKRoy5YtkqSYmBjFx8f76oeGhurJJ59UbGysiouLtWbNGqWkpCgzM7P+PkUd+W4vJosAAGCNS777W89cHo9HhYWF8nq99Tp/JDK+rSa8/4aKiw7qkcTkH94BAADUWm37b0fP3OTWXgAA7HN0GDmRRuw2AwAAJ3N0GDHm2JwRRkYAALDH4WGk8tk0jv4aAACwytm9MHNGAACwztFhxDBnBAAA6xwdRuS7TEMaAQDAFkeHkQqWgwcAwDpHhxFGRgAAsM/RYcQQRgAAsM7RYURcpgEAwDpHh5HKkRG329FfAwAAVjm6F/bd2gsAAKxxdBhRlTDCvBEAAOxwdBjxGxkhjAAAYAVh5DhGRgAAsIMwchxhBAAAOxwdRsRlGgAArHN0GGFkBAAA+xwdRlT1zl7CCAAAVjg6jBhT4fuZLAIAgB0ODyNVL9M4+qsAAMAaR/fA/vNXGRoBAMAGR4cR/zRirxkAADiZo8OIqag6Z4Q0AgCADc4OI1Vup3Hx5F4AAKxwdg/s99BeRkYAALDB0WHE/24aiw0BAMDBnB1GmDMCAIB1jg4jVTFnBAAAO+iBKzEwAgCAFY4PIxXHL9W4SCMAAFjh+DDiW/iMOSMAAFjh+DBiKo6FESawAgBgB2Hk+GIjLjdhBAAAGxwfRiov0zBnBAAAOxwfRgxzRgAAsIowwpwRAACsIowY5owAAGCT48OI72l5jIwAAGCF48OIYQIrAABWEUYqGBkBAMCmOoWRtLQ0bdq0ScXFxcrJyVFSUlKt9ktMTFRZWZk+//zzurzt6cGcEQAArAo4jKSmpmr69OmaMmWK+vTpoyVLligzM1NxcXGn3M/r9Wru3LlatGhRnRt7OvgWPWNkBAAAKwIOI/fee69mz56t2bNna/369brnnnu0detWpaWlnXK/F154QfPnz1d2dnadG3ta+K7SEEYAALAhoDASEhKihIQEZWVl+W3PyspSYmLiSfe77bbb1KFDBz3++OO1ep/Q0FB5PB6/crqY40/tBQAAdgQURiIjIxUcHKz8/Hy/7fn5+YqOjq5xn44dO+qJJ57QiBEjdPTo0Vq9z4QJE1RYWOgreXl5gTQzICfWGXH8XF4AAKyoUw/sW0L9OJfLVW2bJLndbs2fP1+PPfaYvv7661off+rUqfJ6vb4SGxtbl2bWii+McJkGAAArggOpXFBQoPLy8mqjIFFRUdVGSyTJ4/Gof//+6tOnj5577jlJxwKK2+1WWVmZhgwZosWLF1fbr7S0VKWlpYE07ccjjAAAYEVAYaSsrEwrV65UcnKy3n77bd/25ORkvfPOO9XqFxYWqkePHn7bxo4dq8GDB+uGG27Qt99+W7dW16MTIyOWGwIAgEMFFEYkKSMjQ/PmzVNOTo6ys7M1ZswYxcfHa9asWZKk9PR0xcbGauTIkTLGaM2aNX7779q1S0eOHKm23ZYTE1hJIwAA2BBwGFmwYIFatmypiRMnKiYmRrm5uUpJSdGWLVskSTExMYqPj6/3hp42x6e6uJnACgCAFS75uuMzl8fjUWFhobxer4qKiur12I8ufEfNWkcpI3Wk8tZ9Va/HBgDAyWrbfzMcwN00AABY5fgwUsGcEQAArHJ8GPEtB8+cEQAArHB8D8ytvQAA2OX4MFLlSXl2mwEAgEM5PoyYCiawAgBgE2HEd5nG8V8FAABW0AMzZwQAAKscH0Z8TxsmjQAAYIXjw0gl5owAAGCH48OIb9EzwggAAFY4PoywHDwAAHY5PowYwggAAFY5PoxUIowAAGCH48OIYc4IAABWEUa4TAMAgFWODyMnHk1DGAEAwAbHhxEWPQMAwC7CiDk2Z4QsAgCAHYQRHpQHAIBV9MDHr9IwNAIAgB2ODyPcTQMAgF2ODyMnloO33A4AABzK8WGEu2kAALCLMMIEVgAArHJ8D1y5HLzLzcgIAAA2EEaYwAoAgFWEEd/IiOO/CgAArHB8D2wqGBkBAMAmwohhZAQAAJsc3wNXXqZxE0YAALDC8T1wBXNGAACwyvE9sG/OCLf2AgBgBWGkcmSERc8AALDC8T3wiQmsjIwAAGCD48NIxfHLNG53kOWWAADgTI4PIywHDwCAXYQR7qYBAMAqx/fAJyawMjICAIANhJHKB+UxMgIAgBWO74ErWIEVAACrHN8DM2cEAAC76tQDp6WladOmTSouLlZOTo6SkpJOWveiiy7SJ598ooKCAh0+fFjr1q3T+PHj69reescKrAAA2BUc6A6pqamaPn26xo4dq6VLl+quu+5SZmamunXrpq1bt1arf+jQIT333HNatWqVDh06pKSkJL3wwgs6dOiQXnzxxXr5ED8GK7ACAGCfCaQsX77czJgxw2/b2rVrTXp6eq2P8eabb5q5c+fWur7H4zHGGOPxeAJqa23KDY89aKatzjaX3Tmy3o9NoVAoFIqTS23774CGA0JCQpSQkKCsrCy/7VlZWUpMTKzVMXr37q3ExER9/PHHJ60TGhoqj8fjV06XE5dpGBkBAMCGgHrgyMhIBQcHKz8/3297fn6+oqOjT7nv1q1bdeTIEeXk5Oj555/X7NmzT1p3woQJKiws9JW8vLxAmhkQw900AABYVaceuHJtjkoul6vatu8bOHCg+vXrp7vvvlvjx4/XzTfffNK6U6dOldfr9ZXY2Ni6NLNWuJsGAAC7AprAWlBQoPLy8mqjIFFRUdVGS75v8+bNkqTc3Fy1bt1akyZN0uuvv15j3dLSUpWWlgbStDqr4Nk0AABYFdBwQFlZmVauXKnk5GS/7cnJyVq2bFmtj+NyuRQWFhbIW582vhVYuZsGAAArAr61NyMjQ/PmzVNOTo6ys7M1ZswYxcfHa9asWZKk9PR0xcbGauTIkZKksWPHasuWLVq/fr0kKSkpSffdd5+effbZevwYdXdizggjIwAA2BBwGFmwYIFatmypiRMnKiYmRrm5uUpJSdGWLVskSTExMYqPj/fVd7vdmjp1qtq3b6/y8nJ98803euihh/TCCy/U36f4EU7cTRNkuSUAADiTS8fu8T2jeTweFRYWyuv1qqioqF6PffX4NA0e/St99Op8vffkmTFaAwDAT0Ft+2/HT5SoYJ0RAACscnwPbEzlcvDMGQEAwAbCyPGRERY9AwDADsf3wCx6BgCAXY7vgSsIIwAAWOX4HtiwAisAAFYRRirnjLACKwAAVji+B/bdTcNlGgAArHB8D2xYZwQAAKsc3wMzZwQAALscH0YqfA/Kc/xXAQCAFY7vgStHRsIiIiy3BAAAZ3J8GIn/WTdJUvdLkiy3BAAAZ3J8GOl5+aW2mwAAgKM5PowAAAC7HB9GjDG2mwAAgKM5Poy4XNzSCwCATY4PIwAAwC7Hh5G8DV/ZbgIAAI7m+DDyf8+9aLsJAAA4muPDyJGDhyRJe7Ztt9wSAACcyfFhhGfTAABgF2HE8GwaAABscnwPbCqOrTPicjn+qwAAwArH98CVIyNcpgEAwA7CSOXICJdpAACwwvE9sG8CKyuxAgBgBWHEMDICAIBNju+BK7i1FwAAqxwfRribBgAAuxzfA3M3DQAAdhFGfBNYHf9VAABgheN74MrLNKzACgCAHY7vgblMAwCAXYQRJrACAGCV43tgntoLAIBdhJHji565g4IstwQAAGcijBwfGZFYEh4AABscH0bKSkp9P4eEh1lsCQAAzuT4MFJaXKyKo0clSeFNmlhuDQAAzuP4MCJJR8vLJUlBwcGWWwIAgPMQRiTfyAiTWAEAaHh1CiNpaWnatGmTiouLlZOTo6SkpJPWHTZsmLKysrRr1y4dOHBAy5Yt05AhQ+rc4NOhovxYGHEFkc0AAGhoAfe+qampmj59uqZMmaI+ffpoyZIlyszMVFxcXI31L774Yn344YdKSUlRQkKCFi9erPfee0+9e/f+sW2vNxXH76gJYmQEAAArTCBl+fLlZsaMGX7b1q5da9LT02t9jNzcXPPoo4/Wur7H4zHGGOPxeAJqa23LpI/eN9NWZ5voTh1Oy/EpFAqFQnFiqW3/HdDISEhIiBISEpSVleW3PSsrS4mJibU6hsvlksfj0d69e09aJzQ0VB6Px6+cTpWXaRgZAQCg4QUURiIjIxUcHKz8/Hy/7fn5+YqOjq7VMX7/+98rIiJCCxYsOGmdCRMmqLCw0Ffy8vICaWbAKiqYwAoAgC11mrFZuYR6JZfLVW1bTW6++WZNmjRJN910k3bv3n3SelOnTpXX6/WV2NjYujSz1irvpmECKwAADS+ghTUKCgpUXl5ebRQkKiqq2mjJ96Wmpmr27Nm68cYbtWjRolPWLS0tVWlp6Snr1Ccu0wAAYE9AQwFlZWVauXKlkpOT/bYnJydr2bJlJ93v5ptv1iuvvKJbb71VH3zwQd1aehpV3k3jIowAANDgAl5yNCMjQ/PmzVNOTo6ys7M1ZswYxcfHa9asWZKk9PR0xcbGauTIkZKOBZG5c+dq3LhxWr58uVq3bi1JKi4uVmFhYT1+lLqrvEzDyAgAAA0v4DCyYMECtWzZUhMnTlRMTIxyc3OVkpKiLVu2SJJiYmIUHx/vq3/XXXcpJCREM2bM0IwZM3zbX3nlFY0aNaoePsKPxwqsAADYU6eHscycOVMzZ86s8bXvB4xLL720Lm/RoJjACgCAPfS+YgIrAAA2EUZUZQKrmzACAEBDI4yoypyRYMIIAAANjTAiLtMAAGATYUQnloNnAisAAA2P3lesMwIAgE2EEUkVR5nACgCALYQRMYEVAACbCCOqEkbcfB0AADQ0el+xHDwAADYRRsRlGgAAbCKMqOplGsIIAAANjTCiE3fTcJkGAICGRxipIqZzB9tNAADAcQgjkqLOiZckRTRrarklAAA4D2FE0rdfrJIkFe3Za7klAAA4D2FE0sE9+yRJnS8833JLAABwHsKIpPZ9e0mSvJEtLbcEAADnIYxIahYdZbsJAAA4FmFEJ27tBQAADY8wIslUEEYAALCFMKITK7ACAICGRxiRVMHICAAA1hBGJBnmjAAAYA1hRFLpkSO2mwAAgGMRRiT9+6W5tpsAAIBjEUZ0Yhn4IwcPWW4JAADOQxiRZGQkSS63y3JLAABwHsKIqq4zQhgBAKChEUYkHR8YUVjjRnbbAQCAAxFG5L8CqzeqlcWWAADgPIQRnZgzIkmdL+hvsSUAADgPYeR7KipYGh4AgIZEGJEU2ujEXJGKcsIIAAANiTAiqWDLNt/PVYMJAAA4/QgjOjaBdX/+LknSTX942HJrAABwFsLIcc1aR9luAgAAjkQYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABW1SmMpKWladOmTSouLlZOTo6SkpJOWjc6Olqvvfaa1q9fr6NHj+qpp56qc2NPp3f+/LTv55DwMIstAQDAWQIOI6mpqZo+fbqmTJmiPn36aMmSJcrMzFRcXFyN9cPCwrR7925NmTJFX3755Y9u8Onyyfw3fD+3Pvccew0BAMBhAg4j9957r2bPnq3Zs2dr/fr1uueee7R161alpaXVWP+7777T+PHjNW/ePB04cOBHN/h0qTh6Yhn4GyY+aLElAAA4S0BhJCQkRAkJCcrKyvLbnpWVpcTExHprVGhoqDwej19pSN5WkQ36fgAAOFlAYSQyMlLBwcHKz8/3256fn6/o6Oh6a9SECRNUWFjoK3l5efV27NpoGtWqQd8PAAAnq9MEVmOM3+8ul6vath9j6tSp8nq9vhIbG1tvx66tyPi2Df6eAAA4UUBhpKCgQOXl5dVGQaKioqqNlvwYpaWlKioq8isNrf/Qqxv8PQEAcKKAwkhZWZlWrlyp5ORkv+3JyclatmxZvTbMtn5DU2w3AQBqrcfgi9Wf/2/hLBUc6A4ZGRmaN2+ecnJylJ2drTFjxig+Pl6zZs2SJKWnpys2NlYjR4707dOrVy9JUpMmTdSqVSv16tVLpaWlWrduXT19jPrXrHWUItvFqeC7rbabAgA/aNTTf5IkbVzxX+3bsdNya4DABBxGFixYoJYtW2rixImKiYlRbm6uUlJStGXLFklSTEyM4uPj/fb54osvfD/369dPI0aM0ObNm9W+ffsf1/p69sn8N5R0642+33slD9ail1612CIACExE86aEEZx1Ag4jkjRz5kzNnDmzxtdGjRpVbZvL5arL2zS4dZ9k+4WRlHF3a//OfK385/9ZbBUABOLs+P8tUBXPpqnCHK2otu3WqY9ZaAkA1F7Vf/CdLf/4A6oijFRRUVE9jEhS90sHqk2XTurQv28DtwgAaqFqGHETRnD2qdNlmp+qqkvCV3X7M3/2/fznobcof9PmBmoRAPwwvwDCyAjOQoyMVPHdqjU/WCe2W5dq2zoN6Kfzr7vmdDQJAH6QS1ymwdmNMFJFeUmJplw1/JR1ul50gWI6d5AkNY+Jlsvt1t0vPaub/vj/1KZLp4ZoJgD4q3qZhgmsOAtxmeZ79m7brvt7J+kvX3xS4+sJ11yphGuu9P2+69vvfD8PGH6t3kqfdtrbCABVudxV/l3JyAjOQoyM1KDi6FHd3yepVnWj2rfz/Zx0yw080wZAg6uaP5jAirMRIyMnUVFe82TWHzL0wfFyud1aPHueWsTG6LN3PpAkRbaL09D7x+loebkO7tunN//w55M+XPBUDx7seH6CeiZfqn9mPKfS4iN1aiOAnxZu7cXZjjByCpOHDFNo40aqOHpUD733v7Xap9vFF0mSzku6UJJ08+RHa6y3a9N3ylv/lfZs2ab9+bskSU1aNFfy3ber79VDNGfcQzqQv1t7tm7z2y9t9nOSpOLCImU++0KdPheAnxaXi8s0OLsRRk6h6pLKM0aN1dg5M+rt2EMfGOf3e/Ybb+vCG6/z/f7r4+/13K/u0refr5Ikxf+sm+/1qpeDIuPbKrrjucr993/UJXGA9ufvUv4332rA9T9X0ohUzf71fdq/s/6eqlzJHRR00tuhATQgvzt7CSM4+7gk1Xw94Azi8XhUWFgor9eroqIia+24YuwdOrC7QAOu/7nie3T74R3qye7NW9TqnPgfrljFHy6/VhMXvnts/++2qlW7OEnS3u079J+5r+vg3n3qc1Wyul86UIvnvKbQRuHat32HSo+UaNemzdq27iu5XFJE82bqc+XlWvr6myovK1Pp4WJ5oyJ12/QnFN+jm+Y//Afl/vtjyUhHy8tVXlrqa0O4p4l6X3GZwps00UevvFZ/X0gDaN+np9p06aSlr79puylnJJfbrb5XX6Hvvlytgi3bfngHnFbhTSI0JXuhJGnm6N9o44qVllsEHFPb/pswUkcut1vxP+um3/31RdtNaRAVFRVyu2s333n/zny9++Sz+tWTk33bnh811jfaI0kv//Z+RcbH6eDefVr/Sbaat4nRtrXrFdqokVwul0oOH9b5w36uvlcP0Vvp07Tr2+9882ja9+2lX02borefeEqbv1ilqHPa6etPcyRJIeFheuKzjyRJD/a7RI2bejV29nNavegjvT/9xPOUQsLD1DKurXZ+/Y2kY+ezY/++2rZug867OFEjpk6SJL0wZpy+yl5xys/rDgpSWERjhYSFqXB3Qa2+o+9r0qK5IuPjtG3dBvW8fJA2LFuhQ/v211g3LKKxEq65UqsWLtbBPfsCfq+g4GA1i26tVufEacOyFTIVFQpr3FjhnggdyN9dq2P0v+5q3fzHRyRJ/549V1mzXlYjj0eH9u3X0fLyavVdbrcaez06tP+Ab1twWJiaRUed8snYrTu0V7PWUdqw7NPqx6wytyooOFieyJanZQTwbNDI69HkpVm+36dek+rYJ45HtW+nwoI9OlJ00HZTIMJIg+l3bYpumVLzvBDU3cu/vV+3P/uXWtf/R/o0ffqP9/SnnI98296fPlMDR9wob6tISdKqDxerZ/KlKjtSopDwMF+9hwdcppsnP6KeyZdWO+6HL8zR2v8sVcf+CSovLVVoo3CFhIUp6dYbFd4kQpJUsGWb32Wz1x56TP99/1jHcPPkR9V/aIq+Wfm5Fv3Pqwr3NNEv/vS43EFBeufPT8sdFKSSw4eV8ru71bip1++95933iL7+NEe9r7xcg0f/Up9nLtSKt97TuL/NVnjEsff+w+XXKq57N329/DOVHD7sF8aeHnGHyo6UaMdXGyUdG+XytorUr+fMUCOvp8bv8ekRd+jcPr2U889MHdq7/1iIaOpVcEiIrn/kfjVu6lV4kwjt+Gqj+l59RY3HeG3CJG1c8V95I1sqODRUTVo014DhP1e3iy/SB8/MUsI1V2rxnL/qhokPKjgkRDNu/7WMMbr8zpF69y/PaOfGTb5LgNNWZ0uSXhn/kFYv+ljBoaFq17O7Uh9/WJHxbTXrjt/q609zNPaVGeqQ0EfP/vIubc1dq+GPPqBvcj7Xof37VbR7j5q3idHAX6Rq6etvav2SbPVMvlT7duzU9g1f66rf3qXP3vlAe7fl6XfzX1Jk3LHjhoSHyxij9UuWVZtQ3rZbV/W5KllZM2erbbcuiuvRzTf618jr0S1TJirnvUytyvq3Gjf1qv/Qq/V55od+YbVJi+aK7niuNq5YqfMGJqr3VZfrrakZKjtSogtuGKr1nyxXyaFD8kS20I6vvvHtF94kQu6gILmDg1R2pEQlhw6rkderyUv/5auzbskyvTT2977fb578iNxBQZo/4XFJUkznDircVaBD+w8oKCREHc9P0KaVn0uSRjzxuNZ8tESfvf2+32cODg31G/msVPnnKiQ8TPE9ztPSv70pY4wimjVVRYVRcWGhvK0iNXbODH365jtaPOfY9+QODlKT5s1VuLtALpdLoY0aqe81V+hoWbm++L8PfRP0Yzp3VOGu3XIHB6vk0KFTTtyP7niu7n/r2PEnnD9YpcXFfq936N9X7fv01KIXX5UxRu7gIEU0a6aigj3qNihJzaKjdHDvPq1e+JGax8Zo/878Gm9miO54rkoOH9a+7Sd/OnKPwYPkiWyh7AVv+ba1Oide+7bvVHlpqdr36amyklIdLS/XbdOn6oOnZ8lUVOicPj313pPPyhx/PEn7Pj3VNKqVvvjXopO+16kEh4bqaFnZSW+KqHS6LrsTRiyI7dpZCddepUG/vFlv/2m6rntwvO0mwZKKigq9/shk3Zo+scHe86vsFep84fk1vlZy+LDCGjdusLY41YH83WraupXv97x1Xyn2vM6+33PezdTRsjJ9typXQx+8R2GNG53W9hTuLlCTFs3lDgqSJH29PEedLujne33nN98qukP7Gvd9NOkKHTl4SBVHj+rSUSN0zb2/UUVFhZ4cNkIRLZpr5LQpatKieY37Tr/5do1//WVJ0vYNX/stCDln3IPK/fd/dMeMaTpvYKK2rl2vuG5dA/5sW9es0+xf36dJHx0LTR+9Ol+XjLzVr07J4WL933P/o8Gjf6nS4iNq2baNJGlv3g7t+Pobdb+k5iUcct7NVL9rr5IkPfHzmxTdob3WfPyJmrWO0hVj7/S99v70mQoJC1WH/n214q1/augD49S4qdfv79uOr79RTKcO1dp1qnN/YNduhYSFaX/+LrXp3FHSsXP53rRnVVx0SLdNn6rgkBCVHSlR9t/f1qIXX1Xfq6/Q0AfG6dM339WA4ddqf/4uNWsd5XfcV8Y/pMSbrte6T7LV9aILFBbRWG8/MV2/++v/qKykRI8NSlHZkZJaff+1RRg5A7SMa6ug4CDt+vY7NfJ61KFfH639eKkvfVb+qw8AANte/u39WvNRzQt+1lVt+2/upjmNqt6WW1xYpNx//8fv9QcTBqnH4Iv1zWf/VXBYqJq1jtIt6RP13Ze56jrwQr0/fabWL8lW90uSNHBEqla8/b4+e/ufKtqz9/hlhcF+6fqvDz6miGZNdcENQ6slcQAATiUoNNTaezMy8hPkcrt1Tq8eSpv9vHZv2aqMG0fqaFmZ7/UuF12gfj+/Uqs+XKzbpj8hSVr2v/9QcdFBtenSUaXFR1Ry+LASrrlS/30/S3//45/1qycn+4Y0N3+xWs/flqawiAjd/syfdG5C72pt+OJfi9T7isu0+cvVOqfXz3zbf2gi7PeHuX/Iof0HFNGsaa3rAwBqNu++R+o8N+VkuEwDhYSH6WhZ+SknJfUYfLH278zXtrUbTnksd1CQzk3ora2561Ry+LDfay6XS90vvViFu3erbbeuWvH2+yovKfF7vZHXo5JDh/3utPjFn/+gPlcl64NnZmnRi6/6HbP1ueeokderO55/Uo28HhXuLtCT1/9CUeeeo7SXnlNQSLCKC4v05PBf6tx+vTVi6iR9Mv8NvTU1Q5LU+cLzdbS8XHnrNujIwUPyRrXSPa+/rO0bNiqux3n6z1//V+uXLFN5Wbnu/8dfVVFRoek3jZLL7VZYRGN9+98v1bipVy3atlH73j110S03qGjPHnkjI/WPKU+qx+CL1a5XD80c/Rv1vPxStTonToN+dYskaf6Ex3Xr1Mf8Ps8X/1qk71blKjH1em1bu17NWkepfd9e+uRvf9fGFSu1/pNsJd1yg6659zeSpP934eUyFUYX3nidEm8erhfT7vEtvFdx9KhvDoB0bHG+YQ//vsbr39+vW3K4WCFhoSrYss3vUQZV7dy4SdEdz5Uk/ev5F9WkZQttW7NeA4Zfq6j27bQ3b4eaRUepSYvm2v3dVh3cs1dlpaXqfEF/bVi6XO16/0z/mDKtxvkyk68YpqKCvbolfaJ6X3FZje//Q3Z9+512fvOtel5+SZ32r6uF//OKLh9zW70ec+3HS/WPKU/KE9lC4+bPrtMxapob0FBWvP3PBn9i+YLH0nXV7+6Wp2WLBn3fM8nqRR/rZ5cNqvfjzn/4D1r5Xma9HjOQ/tuc6cXj8RhjjPF4PNbbQjlzijs4yPdz09atrLenauly0QVm6IPjTf+hKSa8SUSNdVxud7Vt5/TuacI9TU56zM4Xnn/K9+10QX8z4YM3TId+fXzbul+SZGK7dq723q3PPcfXtrDGjY0kExQSUqfP63K7TYu2bfy2xXbtbC6/a5QJCg4+6X7hTSJMdKcOxtOyhXEHB5kWbduYRz9820xbnW3CGjc2tz/zZ/OnlR+b4Y/cbzr062Paduty0uOljEsz01Zn+/b1teO8zia647n+7XW5/P4MDR79S/PQe/9rOl/Yv/qfs6Agv9/7XZtipq3ONpfdOdLEntfZXPfQPcbTsoVp5PX61Rs4ItVMW51tojt1MI28XhP/s26+c36y7zm0Ubhp262rGfX0Eybp1htMeJMI43K5fJ9n7JwZZtrqbPPQPxeYHoMHVdvf07KFueLXd5pm0a1NRLOmJq5HN7/P3CdliOl3bYrpe/UQ06ZLJ9/30MjrNdfc82vTukN7ExQcbPpfd7X5+X2/NcMffcBcdPNw8/u/zzUTPnjDXHbHSONyu02Xiy7w/TkNCQ8zwWFhJqp9OxPdqYMJCQ8zXZMuMMMfud94o078vbzyt2PMxEXv+s7RuL/N9v3csm2siWjezFc3sl2cCW0Ubq659zdm2upsc8ltI8zv/vqiGfSrW0755zD2vM7m/rfnm0ey3jLN20SbmM4dzcARqebOmU+Z0c8/aVqdE+97z/Gvv2xiu3Y2A4Zfazr072umrc42j3/8gel79RAz6pk/md5XXOb73lu2jTWR7eLMbdOfMNNWZ5u+11xhzun1M9+xHs78u2nZNtZcMfYOc+fMp0xkfFvTuOmJPw+V3+8Tn33k+3vibRVp+l49xFxz729My7i21T5L6qQJZtrqbNOibRvTuKnX3DJlopm2Ott0vyTJr965/fqYxxb/0/S7NsX3nt6oVub+t14z01Znm55DBpuQ8DDfZwkJDzOjnvmTSf/036bVOfHm/OuuMV0SB5iWbWNNSHhYvfz/z+/PZC37b0ZGAJxRgkJCFBwaopJDh32/V73MeCoRzZuddH2W+tRQ7/NTFRIeJlNh1P+6q7Vh6XLtzdtx0rphjRtXG439MVxut8KbNFFxYeGPPlbbbl20b/tOv/Vz4I/LNAAAwKra9t+1W1ITAADgNCGMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArAq23YBAeDwe200AAAC1VNt++6wII5UfJi8vz3JLAABAoDwej4qKik76ukuSabjm1F2bNm1O+UHqwuPxKC8vT7GxsfV+bASGc3Fm4DycOTgXZwbOw4/n8Xi0ffv2U9Y5K0ZGJP3gB/kxioqK+EN2huBcnBk4D2cOzsWZgfNQd7X53pjACgAArCKMAAAAqxwdRkpKSjRp0iSVlJTYborjcS7ODJyHMwfn4szAeWgYZ80EVgAA8NPk6JERAABgH2EEAABYRRgBAABWEUYAAIBVjg4jaWlp2rRpk4qLi5WTk6OkpCTbTTqrDRw4UO+++67y8vJkjNHQoUOr1XnssceUl5enw4cPa/HixerWrZvf66GhoXrmmWe0e/duHTx4UO+8845iY2P96jRr1kxz587V/v37tX//fs2dO1dNmzY9rZ/tbPLQQw9pxYoVKiwsVH5+vt566y117ty5Wj3Oxel1991368svv9SBAwd04MABLVu2TFdeeaVfHc5Bw3vooYdkjNFTTz3lt51zYZ9xYklNTTUlJSVm9OjRpmvXruapp54yRUVFJi4uznrbztZy5ZVXmj/+8Y9m2LBhxhhjhg4d6vf6Aw88YA4cOGCGDRtmunfvbv72t7+ZvLw806RJE1+dGTNmmK1bt5rLLrvM9O7d2yxatMh8/vnnxu12++p88MEHZtWqVeaCCy4wF1xwgVm1apV59913rX/+M6VkZmaakSNHmm7dupmePXua9957z2zevNk0btyYc9GA5ZprrjFXXXWV6dSpk+nUqZOZPHmyKSkpMd26deMcWCr9+vUzmzZtMl988YV56qmnfNs5F2dEsd4AK2X58uVmxowZftvWrl1r0tPTrbftp1BqCiPbt283DzzwgO/30NBQs2/fPjNmzBgjyXi9XlNSUmJSU1N9dWJiYkx5ebkZMmSIkWS6du1qjDHm/PPP99UZMGCAMcaYzp07W//cZ2KJjIw0xhgzcOBAzoXlsmfPHnP77bdzDiyUiIgIs2HDBnPZZZeZxYsX+4URzoX94sjLNCEhIUpISFBWVpbf9qysLCUmJlpq1U9b+/btFRMT4/edl5aW6uOPP/Z95wkJCQoNDfWrs2PHDuXm5vrqXHjhhdq/f79WrFjhq/Ppp59q//79nLuTqBwm3rt3ryTOhQ1ut1s33XSTIiIilJ2dzTmw4Pnnn9f777+vRYsW+W3nXJwZzpoH5dWnyMhIBQcHKz8/3297fn6+oqOjLbXqp63ye63pO2/Xrp2vTklJifbv31+tTuX+0dHR2rVrV7Xj79q1i3N3EhkZGVqyZInWrFkjiXPRkHr06KHs7GyFh4fr4MGDGjZsmNatW6cLL7xQEuegodx0003q27ev+vfvX+01/j6cGRwZRioZY/x+d7lc1bahftXlO/9+nZrqc+5q9txzz6lnz541Ts7mXJx+GzZsUO/evdWsWTMNHz5cr776qgYNGuR7nXNw+rVt21ZPP/20hgwZcsol3TkXdjnyMk1BQYHKy8urpdWoqKhq6Rj1Y+fOnZJ0yu98586dCgsLU7NmzU5Zp3Xr1tWO36pVK87d9zzzzDO69tprdemllyovL8+3nXPRcMrKyvTNN99o5cqVevjhh/Xll19q3LhxnIMGlJCQoNatW2vlypUqKytTWVmZLrnkEv3ud79TWVmZ73viXNhnfeKKjbJ8+XLz/PPP+21bs2YNE1jrqZxsAuv999/v+z0kJKTGSWI33nijr050dHSNk8T69+/vq3P++eczSex75dlnnzXbtm0zHTt2rPF1zoWdsnDhQjNnzhzOQQOWJk2amO7du/uVFStWmLlz55ru3btzLs6cYr0BVkrlrb2jRo0yXbt2NRkZGaaoqMjEx8dbb9vZWiIiIkyvXr1Mr169jDHGjB8/3vTq1ct3u/QDDzxg9u3bZ6677jrTvXt389prr9V4+9yWLVvM4MGDTe/evc3ChQtrvH3uiy++MAMGDDADBgwwX375JbfPVSnPP/+82bdvn7n44otN69atfSU8PNxXh3Nx+suUKVNMUlKSadeunenRo4eZPHmyKS8vN5dffjnnwHL5/t00nIszolhvgLWSlpZmvv32W3PkyBGTk5Pjd+sjJfAyaNAgU5PKfwlKMo899pjZvn27KS4uNh999JHvXyaVJSwszDzzzDOmoKDAHDp0yLz77rumbdu2fnWaN29u5s2bZw4cOGAOHDhg5s2bZ5o2bWr9858p5WRGjhzpV49zcXrLSy+95Pv/S35+vvnwww99QYRzYLd8P4xwLuwX1/EfAAAArHDkBFYAAHDmIIwAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACw6v8DQCb+Pj3j1CQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MOEModelForCausalLM(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MOEMLP(\n",
       "          (mlps): ModuleList(\n",
       "            (0-2): 3 x GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autoreload bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/sonia/miniconda3/envs/great/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt/data/sonia/miniconda3/envs/great/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "import transformers\n",
    "from torch import nn\n",
    "import torch\n",
    "from be_great.multihead_models import MOEModelForCausalLM\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm \n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "dgpt2 = transformers.AutoModelForCausalLM.from_pretrained('distilgpt2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "special_tokens_dict = {\"bos_token\": \"<BOS>\", 'eos_token': '<EOS>'}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# Preprocess the data: Convert each row to a string\n",
    "def row_to_string(row):\n",
    "    return \", \".join([f\"{col} is {val}\" for col, val in row.items()]) + \".\"\n",
    "def row_to_sentences(row):\n",
    "    return '. '.join([str(col).strip() + \" is \" + str(val).strip() for col, val in zip(row.index, row.values)])\n",
    "def row_to_col_sentences(row):\n",
    "    return [str(col).strip() + \" is \" + str(val).strip() + '.<EOS>' for col, val in zip(row.index, row.values)]\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, cols=None, max_col_length=10, do_moe_format=True):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cols = cols # \"None\" for all cols, else a list of desired cols' names\n",
    "        self.max_col_length = max_col_length\n",
    "        self.do_moe_format = do_moe_format\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.cols is None:\n",
    "            text = row_to_col_sentences(data.iloc[idx])\n",
    "        else:\n",
    "            text = row_to_col_sentences(data[self.cols].iloc[idx]) # ['age is 39.', 'workclass is State-gov.', ...]\n",
    "        if self.do_moe_format:\n",
    "            tokenized_text = self.tokenizer(text, truncation=True, max_length=self.max_col_length, padding='max_length', return_tensors=\"pt\")\n",
    "            prompt = torch.full((1,), #batch_size x token\n",
    "                                self.tokenizer.bos_token_id)\n",
    "            return prompt, tokenized_text.input_ids.squeeze()\n",
    "        else:\n",
    "            text = tokenizer.bos_token + ''.join(text)\n",
    "            # print(text)\n",
    "            tokenized_text = self.tokenizer(text, truncation=True, padding='longest', return_tensors='pt')\n",
    "            return tokenized_text.input_ids.squeeze(), tokenized_text.attention_mask.squeeze()\n",
    "            \n",
    "            \n",
    "# Load the dataset\n",
    "file_path = 'adult.csv'  # Update this with the correct path\n",
    "data = pd.read_csv(file_path)\n",
    "text_data = data.apply(row_to_col_sentences, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MOEModelForCausalLM(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MOEMLP(\n",
       "          (mlps): ModuleList(\n",
       "            (0-14): 15 x GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "MOEModelForCausalLM(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MOEMLP(\n",
       "          (mlps): ModuleList(\n",
       "            (0-14): 15 x GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n",
      "range(0, 15)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m prompt \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 24\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmulticol_forward(input_ids\u001b[38;5;241m=\u001b[39mprompt, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# outputs = model.debug_forward(ins['input_ids'].to(device), ins['attention_mask'].to(device), labels=labels)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/be_great/be_great/multihead_models.py:177\u001b[0m, in \u001b[0;36mMOEModelForCausalLM.<locals>.MOEModelForCausalLM.multicol_forward\u001b[0;34m(self, input_ids, attention_mask, labels, cols_iterator, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m cols_iterator:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m i\n\u001b[0;32m--> 177\u001b[0m     transformer_outputs \u001b[38;5;241m=\u001b[39m transformer(prompt, attention_mask\u001b[38;5;241m=\u001b[39mmask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    178\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    179\u001b[0m     lm_logits \u001b[38;5;241m=\u001b[39m lm_head(hidden_states)\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/great/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/great/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/great/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:888\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    876\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    877\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    878\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m         output_attentions,\n\u001b[1;32m    886\u001b[0m     )\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m block(\n\u001b[1;32m    889\u001b[0m         hidden_states,\n\u001b[1;32m    890\u001b[0m         layer_past\u001b[38;5;241m=\u001b[39mlayer_past,\n\u001b[1;32m    891\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    892\u001b[0m         head_mask\u001b[38;5;241m=\u001b[39mhead_mask[i],\n\u001b[1;32m    893\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m    894\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[1;32m    895\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    896\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    897\u001b[0m     )\n\u001b[1;32m    899\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/great/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/great/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/great/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:427\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    425\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    426\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> 427\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[1;32m    429\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/great/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/great/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/be_great/be_great/multihead_models.py:31\u001b[0m, in \u001b[0;36mMOEMLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# print('generate with mlp', self.col.value)\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol\u001b[38;5;241m.\u001b[39mvalue](hidden_states)\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/great/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/great/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/great/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:356\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    354\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_fc(hidden_states)\n\u001b[1;32m    355\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(hidden_states)\n\u001b[0;32m--> 356\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(hidden_states)\n\u001b[1;32m    357\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/great/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/great/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/great/lib/python3.11/site-packages/transformers/pytorch_utils.py:103\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    102\u001b[0m     size_out \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnf,)\n\u001b[0;32m--> 103\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39maddmm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[1;32m    104\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(size_out)\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_experts = 15\n",
    "dgpt2copy = MOEModelForCausalLM(dgpt2, num_experts=num_experts)\n",
    "model = dgpt2copy # don't forget to change tokenizer name and optimizer too\n",
    "\n",
    "model.train()\n",
    "# Move the model to the device (GPU if available)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "dataset = TextDataset(text_data, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Set up the optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
    "\n",
    "lossesmoe = []\n",
    "for epoch in range(1):  # Train for 3 epochs\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        prompt, labels = batch\n",
    "        prompt = prompt.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model.multicol_forward(input_ids=prompt, labels=labels)\n",
    "        # outputs = model.debug_forward(ins['input_ids'].to(device), ins['attention_mask'].to(device), labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        lossesmoe.append(loss.item())\n",
    "        if len(lossesmoe) % 500 == 0:\n",
    "            torch.save(model.state_dict(), f'./ckpts/moe/dgpt2/adult-1col/debug{len(lossesmoe)}.pt')\n",
    "            try:\n",
    "                plt.close()\n",
    "            except:\n",
    "                pass\n",
    "            plt.plot(lossesmoe)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48842/48842 [16:29<00:00, 49.34it/s]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>age is 39.<EOS>workclass is Private.<EOS>fnlwgt is 124778']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hdd2/sonia/miniconda3/envs/great/lib/python3.11/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb537843d10>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = dgpt2 # don't forget to change tokenizer name and optimizer too\n",
    "model.train()\n",
    "# Move the model to the device (GPU if available)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "dataset = TextDataset(text_data, tokenizer, None, do_moe_format=False)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Set up the optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "# num_training_steps = len(dataloader) * 1  # Number of epochs\n",
    "# lr_scheduler = LinearLR(optimizer, total_iters=num_training_steps)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(1):  # Train for 3 epochs\n",
    "    for batch in tqdm(dataloader):\n",
    "        # print(batch)\n",
    "        input_ids, attention_mask = batch\n",
    "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "print(tokenizer.batch_decode(model.generate()))\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>age is 24.<EOS>workclass is Private.<EOS>fnlwgt is 124778.<EOS>education is HS-grad.<EOS>education-num is 9.<EOS>marital-status is Never-married.<EOS>occupation is Craft-repair.<EOS>relationship is Own-child.<EOS>race is White.<EOS>sex is Male.<EOS>capital-gain is 0.<EOS>capital-loss is 0.<EOS>hours-per-week is 40.<EOS>native-country is United-States.<EOS>income is <=50K.<EOS>income is <=50K.<EOS>income is <=50K.<EOS>income is <=50K'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(model.generate(max_length=130))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('./ckpts/dgpt2/adult-allcol/', exist_ok=True)\n",
    "torch.save(model.state_dict(), f'./ckpts/dgpt2/adult-allcol/{len(losses)}.pt')\n",
    "plt.plot(losses)\n",
    "plt.savefig('./ckpts/dgpt2/adult-allcol/losses.png')\n",
    "\n",
    "samples = []\n",
    "for i in tqdm(range(10000)):\n",
    "    samples.append(tokenizer.batch_decode(model.generate(do_sample=True, num_beams=1, max_length=140))[0])\n",
    "    if i %1000 == 0:\n",
    "        with open('./ckpts/dgpt2/adult-allcol/samples.txt', 'a') as f:\n",
    "            f.write('\\n'.join(samples))\n",
    "        samples = []\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hdd2/sonia/miniconda3/envs/great/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MOEModelForCausalLM(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MOEMLP(\n",
       "          (mlps): ModuleList(\n",
       "            (0-14): 15 x GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 768)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgpt2 = transformers.AutoModelForCausalLM.from_pretrained('distilgpt2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "special_tokens_dict = {\"bos_token\": \"<BOS>\", 'eos_token': '<EOS>'}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "num_experts = 15\n",
    "dgpt2copy = MOEModelForCausalLM(dgpt2, num_experts=num_experts)\n",
    "model = dgpt2copy # don't forget to change tokenizer name and optimizer too\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.load_state_dict(torch.load('/hdd3/sonia/be_great/ckpts/moe/dgpt2/adult-allcol/jul21/48000.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>age is 50.<EOS>workclass is Private.<EOS>fnlwgt is 266061.<EOS>education is Assoc-voc.<EOS>education-num is 11.<EOS>marital-status is Married-civ-spouse.<EOS>occupation is Adm-clerical.<EOS>relationship is Not-in-family.<EOS>race is White.<EOS>sex is Male.<EOS>capital-gain is 0.<EOS>capital-loss is 0.<EOS>hours-per-week is 40.<EOS>native-country is United-States.<EOS>income is <=50K.<EOS>income is <=50K.<EOS>native-country is United-States.<EOS>income is <=50'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(model.generate(do_sample=True, num_beams=1, max_length=140))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for i in tqdm(range(10000)):\n",
    "    samples.append(tokenizer.batch_decode(model.generate(do_sample=True, num_beams=1, max_length=140))[0])\n",
    "    \n",
    "with open('/hdd3/sonia/be_great/ckpts/moe/dgpt2/adult-allcol/jul21/samples.txt', 'w') as f:\n",
    "    f.write('\\n'.join(samples))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
